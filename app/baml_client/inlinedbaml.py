###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml-py
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {
    
    "choose_tools.baml": "\nclass WeatherAPI {\n  city string @description(\"the user's city\") @stream.not_null\n  timeOfDay string @description(\"As an ISO8601 timestamp or right now / atm\") @stream.not_null\n//   weather string @description(\"Come up with something weather related such as 'extremely rainy', 'shockingly sunny' or something other funny\")\n}\n\n\nclass USPresidentAPI {\n  name string @description(\"The name of the US president that one wants to extract data for\") @stream.not_null\n  note string | null @description(\"A note for the data extraction, might be null / empty\")\n//   timezone string @description(\"The made up time zone like; 'Galactica timezone' or something other from startrek\")\n}\n\nclass AssistantMsg {\n  role \"assistant\"\n  content string\n}\n\nclass ChatMsg {\n  role \"user\" | \"assistant\"\n  content string\n}\n\n\n// class AssistantMsgClass {\n//   message_type \"greeting\" | \"conversation\" | \"farewell\" @stream.not_null\n//   message string @stream.with_state @stream.not_null\n// }\n\n\nfunction ChatWithTools(messages: ChatMsg[]) -> string | AssistantMsg | WeatherAPI | USPresidentAPI {\n  client Gemini20Flash\n  prompt #\"\n    {{_.role(\"system\")}}\n    You are a helpful and knowledgeable AI assistant engaging in a conversation.\n    Your responses should be:\n    - Clear and concise\n    - Accurate and informative\n    - Natural and conversational in tone\n    - Focused on addressing the user's needs\n    - Always choose any of the other tools than the conversational message if possible\n\n    Use the USPresidentAPI if the user asks for the time at a specific location\n    Use the WeatherAPI if the user asks for the weather for a location\n    Always use AssistantMsg otherwise\n\n    # Requirement\n    Analyze the conversation history and double check your conclusion for which tool to choose to be consistent.\n    Always choose a tool for the output\n\n    {{ ctx.output_format }}\n\n    {% for m in messages %}\n    {{ _.role(m.role)}}\n    {{m.content}}<br>\n    {% endfor %}\n\n  \"#\n}\n\ntest TestmultiTool1 {\n  functions [ChatWithTools]\n  args {\n    messages [\n      {\n        role \"user\"\n        content \"What is the weather like in Brisbane right now?\"\n      }\n    ]\n  }\n}\n\ntest TestmultiTool2 {\n  functions [ChatWithTools]\n  args {\n    messages [\n      {\n        role \"user\"\n        content \"Abraham Lincoln. I think he was smart but dont know\"\n      }\n    ]\n  }\n}\n\ntest TestmultiTool3 {\n  functions [ChatWithTools]\n  args {\n    messages [\n      {\n        role \"user\"\n        content \"Hey man, how you doing? Are you a man or a woman?\"\n      }\n    ]\n  }\n}\n\n\ntest TestmultiTool4 {\n  functions [ChatWithTools]\n  args {\n    messages [\n      {\n        role \"user\"\n        content \"Hey man, is it probably raining at 17:00 the 24.05.25 in Stonehenge?\"\n      },\n      {\n        role \"assistant\"\n        content \"To check the weather in Stonehenge on May 24, 2025, at 17:00, I'll need to access a weather tool. Would you like me to do that?\"\n      },\n      {\n        role \"user\"\n        content \"Yes, please!\"\n      }\n    ]\n  }\n}",
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> Llama3 {\n  provider ollama\n  retry_policy Constant\n  options {\n    model \"llama3.2:1b\"\n    temperature 0.6\n  }\n}\n\nclient<llm> Mistral {\n  provider ollama\n  retry_policy Constant\n  options {\n    model \"mistral-small:24b\"\n    temperature 0.6\n  }\n}\n\nclient<llm> GeminiFlash {\n  provider google-ai\n  options {\n    model \"gemini-1.5-flash\"\n    api_key env.GOOGLE_API_KEY\n    generationConfig {\n    temperature 0.75\n    }\n  }\n}\n\nclient<llm> Gemini20Flash {\n  provider google-ai\n  options {\n    model \"gemini-2.0-flash\"\n    api_key env.GOOGLE_API_KEY\n    generationConfig {\n    temperature 0.75\n    }\n  }\n}\n\nclient<llm> Gemini20FlashLite {\n  provider google-ai\n  options {\n    model \"gemini-2.0-flash-lite-preview-02-05\"\n    api_key env.GOOGLE_API_KEY\n    generationConfig {\n    temperature 0.75\n    }\n  }\n}\n\nclient<llm> Azure4oClient {\n  provider azure-openai\n  options {\n    resource_name env.AZURE_RESOURCE\n    deployment_id env.AZURE_MODEL \n    api_version \"2024-10-01-preview\"\n    api_key env.AZURE_OPENAI_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [Gemini20Flash]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [Gemini20Flash]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "example_resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client Gemini20Flash // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest phs_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      PÃ¥l HS\n      phs@hotmail.com\n\n      Experience:\n      - Founder at Nothing\n      - CV Engineer at DontKnowIt\n      - Systems Engineer at Ferrari\n\n      Skills:\n      - Python snakes\n      - C++\n    \"#\n  }\n}\n",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.77.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
}

def get_baml_files():
    return file_map